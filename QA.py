# -*- coding: utf-8 -*-
"""QA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18ddFEUpKhobs7bhruOuHqwLZ6YvgLZgI

# Importing packages
"""

# for File name retrieval
import os
import glob
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np

# for query preprocessing
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# for most relevant chunk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

#NLU generation
import subprocess

"""# QA"""

class main_1:
  def __init__(self):
      self.chat = True
      self.select_course = False

      self.course_name = None
      self.passages = None
      self.course_link = None

      self.system = 1 # 1 = Answer, 2 = sources, 3 = answer + sources

      self.tokenizer, self.model = self.preload()

      self.file_embeddings = None

  # to preload and set all the models, embeddings required for QA
  def preload(self):
      tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-V2")
      model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-V2")

      return tokenizer, model

  def read_file(self, file_path):
      with open(file_path, "r", encoding="utf-8") as file:
        return file.read()[:100]

  # To get the most relevant file for course selection
  def get_most_rel_filename(self, query):
      query_embedding = self.model(**self.tokenizer(query, return_tensors="pt"))['pooler_output'].detach().numpy()
      similarity_scores = {}

      directory = "HW courses"
      for filename in os.listdir(directory):
        if filename.endswith(".txt"):
          file_path = os.path.join(directory, filename)
          text = self.read_file(file_path)
          text_embd = self.model(**self.tokenizer(text, return_tensors="pt"))['pooler_output'].detach().numpy()
          similarity = cosine_similarity(query_embedding, text_embd)
          similarity_scores[filename] = similarity[0][0]

      most_similar_file_name = max(similarity_scores, key=similarity_scores.get)

      return most_similar_file_name

  # set the course name & link for later retrieval
  def set_course(self,c_name, c_link):
    self.course_name = c_name
    self.course_link = c_link

  # Retrieve contents from the selected file
  def get_file_contents(self, file_name):
      file_contents = open(f"HW courses/{file_name}","r").read()

      course_name = ''
      course_link = ''

      with open(f"HW courses/{file_name}", 'r') as file:
          for line in file:
              # Skip empty lines
              if line.strip():
                  if not course_name:
                      course_name = line.strip().split("Course name: ")[1]
                  elif not course_link:
                      course_link = line.strip().split("Course link: ")[1]
                      break  # Stop reading after finding the second non-empty line

      self.set_course(course_name, course_link)
      return file_contents

  # Splitting the file contents into chunks
  def split_chunks(self, file_name):
      text_splitter = RecursiveCharacterTextSplitter(separators=[ "end of passage"],chunk_size=550, chunk_overlap=100)

      chunks = text_splitter.split_text(self.get_file_contents(file_name).lower())

      return chunks

  # preprocess the query for better fit
  def preprocess_query(self, query):

    # Tokenizing the query
    tokens = word_tokenize(query)

    # Removing the stop words
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]

    # joing the words without stop words
    processed_query = ' '.join(filtered_tokens)

    return processed_query

  # To get the most relevant passage to the query
  def most_rel_passage(self, query):

    # Combining the query and passages for TF-IDF vectorization
    documents = [self.preprocess_query(query)] + self.passages

    # TF-IDF Vectorization
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(documents)

    # Calculating cosine similarity between the query and each passage
    cosine_similarities = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1:]).flatten()

    # Getting the index of the most similar passage
    most_similar_index = cosine_similarities.argmax()

    #check
    # print(cosine_similarities)

    # accessing the most relevant passage
    most_relevant_passage = self.passages[most_similar_index]

    return most_relevant_passage

  # setting/modifying the select_course variable so that we will be able to switch conversation between different courses
  def set_course_selection(self, x):

    self.select_course = x

  #switching between system types
  def switch_sys(self, sys_type):
    if sys_type == "/sys_a":
      self.system = 1
      response = "System has been switched to Answer Only Mode"
    elif sys_type == "/sys_s":
      self.system = 2
      response = "System has been switched to Sources Only Mode"
    else:
      self.system = 3
      response = "System has been switched to Answer and Sources Mode"

    return response

  def run_ollama(self, passage, query):
      # Define the command to be executed
      command = f'ollama run Cbot "Content : \'{passage}\' Query : \'{query}\'"'

      # Execute the command
      process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

      # Wait for the command to complete
      stdout, stderr = process.communicate()

      response = ""

      # Check if there was an error
      if process.returncode != 0:
          response = "Error executing command:"
          response += stderr.decode()
      else:
          # The output of the command
          response = stdout.decode()

      return response

  # To print the bot created response
  def print_response(self, response="",link=""):

      print("\n\nCbot : "+ response+link)

  def QA(self):
    # Loop for chat
    while self.chat:
      response = "Hi, \nI'm Cbot, \nWhich course would you talk about today?"
      query = input().lower()

      # Check to end chat
      if query in ["/end", "/stop", "/bye"]:
        self.chat = False
        response = "Thanks for spending time with me, Hope I was helpful. See you again"
        self.print_response(response)
        break
      if query in ["/sys_a","/sys_as","/sys_s"]:
        response = self.switch_sys(query)
        self.print_response(response)
        continue

      if self.select_course == False:
          if query in ["/select_course", "/select course"]:
              self.set_course_selection(False)
              response = "Hi, \nI'm Cbot, \nWhich course would you talk about today?"
              self.print_response(response)
              continue

          file_name = self.get_most_rel_filename(self.preprocess_query(query))
          self.passages = self.split_chunks(file_name)
          response = "How can I help you with the "+str(file_name)[:-4]+" course."
          self.set_course_selection(True)

      else:
          if query in ["/select_course", "/select course"]:
                self.set_course_selection(False)
                response = "Hi, \nI'm Cbot, \nWhich course would you talk about today?"
                self.print_response(response)
                continue
          response = self.most_rel_passage(query)[15:]
          #Run ollama
          # response = self.run_ollama(response, query)


      if self.system == 3:
        self.print_response(response+"\n\n\nSources:\n",self.course_link)
      elif self.system == 2:
        self.print_response("\n\n\nSources:\n",self.course_link)
      else:
        self.print_response(response)
      response = ""

main_1().QA()
