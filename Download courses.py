# -*- coding: utf-8 -*-
"""Download courses.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18ddFEUpKhobs7bhruOuHqwLZ6YvgLZgI
"""

"""# Importing Packages"""

# For Web scraping
import requests
from bs4 import BeautifulSoup
import requests
from urllib.parse import urljoin
import html
import re
from requests.exceptions import ConnectionError
import os

"""# Downloading courses data from HeriotWatt Uni Site"""


class web_scraping:
    def __init__(self, site):
        self.base_url = "https://search.hw.ac.uk"
        self.site_link = site
        self.raw_content = self.get_raw_site_contents()

    def get_raw_site_contents(self):
        # setting up a user agent in your requests to make it appear as a regular browser request
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
        raw_contents = requests.get(url=self.site_link, headers=headers)

        return raw_contents.text

    def get_abs_url(self, redirect_link):
        # Base URL of the website
        base_url = "https://www.hw.ac.uk"

        # Relative URL from our HTML snippet
        relative_url = redirect_link

        # Decode HTML entities in the relative URL
        relative_url = html.unescape(relative_url)

        # Combine the base URL and relative URL to create the absolute URL
        absolute_url = urljoin(base_url, relative_url)

        return absolute_url

    # Here is where the raw contents of the website will be processed into passages
    def get_passages(self):
        soup = BeautifulSoup(self.raw_content, "html.parser")

        # Finding all headings and their contents
        all_headings = ["h1", "h2", "h3", "h4", "h5", "h6"]
        headings = soup.find_all(all_headings)
        passages = []  # Just a Container to store all passages

        for heading in headings:

            # finding all siblings until the next heading
            sib = ['p', 'ul', 'ol', 'table', 'dl']  # all the siblings we'll be considering

            paragraph = heading.find_next_sibling(sib)
            passage_txt = ""
            # para_name = "h9"
            # print(heading.get_text())
            while paragraph and paragraph.name not in headings:
                # Exxtracting data from tables
                if paragraph.name == 'table':
                    table = paragraph
                    caption = table.find_all('caption')
                    for cap in caption:
                        passage_txt += cap.get_text() + " : \n"
                    rows = table.find_all("tr")
                    for row in rows:
                        cols = row.find_all(["th", "td"])
                        row_txt = ""
                        for col in cols:
                            row_txt += col.get_text().strip() + " : "
                        passage_txt += row_txt + "\n"
                    paragraph = paragraph.find_next_sibling(sib)
                    passage_txt += "\n"
                    break
                # Extracting data from description list
                elif paragraph.name == "dl":
                    dts = paragraph.find_all("dt")
                    for dt in dts:
                        dd = dt.find_next_sibling("dd")
                        passage_txt += dt.get_text().strip() + " : " + dd.get_text().strip() + "\n"
                    paragraph = paragraph.find_next_sibling(sib)
                    break
                elif paragraph.name == 'ul' or paragraph.name == 'ol':
                    items = paragraph.find_all('li')
                    passage_txt += "\n".join([f"\t\tâ‚ {item.get_text()}" for item in items])
                    passage_txt += "\n"
                    paragraph = paragraph.find_next_sibling(sib)
                    break
                else:
                    passage_txt += paragraph.get_text().strip() + "\n"
                    paragraph = paragraph.find_next_sibling(sib)
                    break

            heading_txt = heading.get_text().strip().lower()
            if passage_txt != "" and heading_txt != "contact" and heading_txt != "apply":
                passages.append(
                    {"heading": heading_txt, "content": f"{heading_txt.upper()} of the course :\n{passage_txt}"})

        # Initialize variables for the links
        apply_link = None
        contact = None

        all_anchor_tags = soup.find_all('a')

        # Iterate through anchor tags and check their text content
        for anchor_tag in all_anchor_tags:
            if "Find out how to apply" in anchor_tag.get_text() or "Apply now" in anchor_tag.get_text():
                apply_link = anchor_tag['href']
            elif "Contact us" in anchor_tag.get_text():
                contact = anchor_tag['href']

        # Check if the links were found
        if apply_link:
            passages.append({"heading": "Apply",
                             'content': f"\nFollow the provided link below to apply for the course : \n{self.get_abs_url(apply_link)}"})
        else:
            passages.append(
                {"heading": "Apply",
                 "content": "\nHow to Apply for the course : \nSorry, \nthe requested details couldn't be found"})

        if contact:
            passages.append({"heading": "Contact", 'content': f"\nContact for the course : \n{contact}"})
        else:
            passages.append(
                {"heading": "Contact",
                 "content": "\nContact for the course : \nSorry, \nthe requested details couldn't be found"})

        content = ""

        for passage in passages:
            content += passage['content'] + "\n\nEnd of passage\n\n"

        return content


class course_site_retrieval:
    def __init__(self):
        # default site where we can see all courses available (Global College, Postgraduate, Research, Undergraduate all levels ate selected)
        self.parent_site = "https://search.hw.ac.uk/s/search.html?gscope1=uk%2Conline%7C&profile=programmes&f.Level%7Clevel=Undergraduate&f.Level%7Clevel=Research&f.Level%7Clevel=Postgraduate&f.Level%7Clevel=Global+College&collection=heriot-watt~sp-programmes&start_rank=1"
        self.course_data = []
        self.process_data()

    def get_abs_url(self, redirect_link):
        # Base URL of the website
        base_url = "https://search.hw.ac.uk"

        # Relative URL from our HTML snippet
        relative_url = redirect_link

        # Decode HTML entities in the relative URL
        relative_url = html.unescape(relative_url)

        # Combine the base URL and relative URL to create the absolute URL
        absolute_url = urljoin(base_url, relative_url)

        return absolute_url

    def process_course_name(self, name):
        text = name

        # Remove extra spaces and non-breaking spaces
        cleaned_text = re.sub(r'\s+', ' ', text)
        cleaned_text = re.sub(r'&nbsp;', '', cleaned_text)
        cleaned_text = re.sub(r'>', '', cleaned_text)
        cleaned_text = re.sub(r'/', ' ', cleaned_text)

        # Strip leading and trailing whitespace
        cleaned_text = cleaned_text.strip()

        return cleaned_text

    def fetch_url(self, link):

        # maximum number of retry attempts
        response = None
        max_retry_attempts = 3

        # Initialize a variable to keep track of retry attempts
        retry_count = 0

        while retry_count < max_retry_attempts:
            try:
                # Send a GET request
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
                response = requests.get(url=link, headers=headers)

                # Check if the request was successful
                if response.status_code == 200:
                    break  # Exit the loop if the request was successful
                else:
                    print(f"Request failed with status code {response.status_code}")

            except ConnectionError as e:
                print(f"ConnectionError: {e}")
                retry_count += 1
                if retry_count < max_retry_attempts:
                    print(f"Retrying ({retry_count}/{max_retry_attempts})...")
                else:
                    print("Max retry attempts reached. Exiting...")
                    break
        return response

    def save_data(self, i):

        # Defining what the folder name is, and what the file contents are
        folder_name = "HW courses"
        file_content = f"{i['course Information']}"
        file_name = str(i["course name"]) + ".txt"

        # Checking whether the folder exists, create it if not
        if not os.path.exists(folder_name):
            os.makedirs(folder_name)

        # Construct the file path
        file_path = os.path.join(folder_name, file_name)

        # Write content to the file with UTF-8 encoding
        with open(file_path, 'w', encoding='utf-8') as file:
            file.write(file_content)

    def process_data(self):

        # Replace this with the URL of the webpage you want to scrape
        url = self.parent_site

        count = 1

        # continue until all the courses are scraped
        while True:

            # Sending an HTTP GET request to the course search webpage
            response = self.fetch_url(url)

            # Check if the request was successful
            if response.status_code == 200:

                # Parse the HTML content of the page with BeautifulSoup
                soup = BeautifulSoup(response.text, "html.parser")

                # Find the table element with the specified class
                table = soup.find("table", class_="hw_course-search__courses")

                if table:
                    print(f"Retrieving page {count}/26", end="")
                    # print("-->Courses Table found on the page.\n\t\tRetrieving data, Please wait.....")
                    print()
                    count = count + 1

                    # if count < 10:
                    #   continue

                    # Find all rows (tr elements) in the table body
                    rows = table.find("tbody").find_all("tr")

                    # Iterate through each row
                    for row in rows:

                        # Extract course name and course link
                        name_element = row.find("a")
                        name = self.process_course_name(name_element.get_text(strip=False))
                        course_link = self.get_abs_url(name_element["href"])

                        # Sending an HTTP GET request to the course webpage
                        course_response = self.fetch_url(course_link)

                        # Perform soup.get_text() on the link to get the content
                        if course_response.status_code == 200:
                            # course_soup = BeautifulSoup(course_response.text, "html.parser")
                            content = web_scraping(course_link).get_passages()

                            c = f"\n\nCourse name: {name}\n\n Course link: {course_link}\n\n course Information: {content}"

                            # Create a dictionary for the course data
                            course_data = {"course name": name, "course Information": c}

                            data = {"name": name, "course_link": course_link, "content": content}

                            # Append the dictionary to the list
                            # self.course_data.append(data)
                            self.save_data(course_data)
                else:
                    print("Table not found on the webpage.")
                    break

            else:
                print("Failed to retrieve the webpage.")
                break

            #if count > 2:
                #break

            # finding the next button
            next_page_link = soup.find("a", class_="hw_course-search__pagination-link--next")

            # if next button exists continue web scraping
            if next_page_link:
                url = self.get_abs_url("s/" + next_page_link["href"])
            else:
                print("End of retrieval")
                break

    def get_course_data(self):

        return self.course_data


all_course_data = course_site_retrieval().get_course_data()
